{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3bd9735",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d046b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%run C:\\Users\\PC\\Downloads\\project\\project\\EDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b44c6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data = \u001b[43mdf\u001b[49m.copy()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e34c06",
   "metadata": {},
   "source": [
    "## Handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f49c5",
   "metadata": {},
   "source": [
    "### 1. Drop features\n",
    "\n",
    "From our previous inspection:\n",
    "- ``user_id`` -> Meaningless\n",
    "- ``ZONE1`` -> 92% missing\n",
    "- ``ZONE2`` -> ~94% missing\n",
    "- ``MRG`` -> Only has a single value (`NO`) for all rows, so it does not contain any information.\n",
    "\n",
    "When a feature has > 90% missing values, it contians very little information. Moreover, imupting such a large proportion would only introduce noise and bias, making the feature meaningless. Lastly, keeping it would only increase the model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3efcb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['ZONE1', 'ZONE2', 'MRG', 'user_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8066c",
   "metadata": {},
   "source": [
    "### 2. Impute numeric features\n",
    "\n",
    "Numeric features with moderate missingness (30% ~ 49%):\n",
    "- `MONTANT`, `FREQUENCE_RECH`, `REVENUE`, `ARPU_SEGMENT`, `FREQUENCE`, `DATA_VOLUME`, `ON_NET`, `ORANGE`, `TIGO`, `FREQ_TOP_PACK`\n",
    "\n",
    "The strategy will be used is the **median** imputation instead of mean:\n",
    "- Median is robust to outliers.\n",
    "- Keeps the distribution reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efd45248",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['MONTANT', 'FREQUENCE_RECH', 'REVENUE', 'ARPU_SEGMENT', \n",
    "                    'FREQUENCE', 'DATA_VOLUME', 'ON_NET', 'ORANGE', 'TIGO', 'FREQ_TOP_PACK']\n",
    "\n",
    "for col in numeric_features:\n",
    "    median_val = data[col].median()\n",
    "    data[col].fillna(median_val, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2528a8",
   "metadata": {},
   "source": [
    "### 3. Impute categorical features\n",
    "\n",
    "Categorical features with missing values:\n",
    "- `REGION` -> ~39% missing \n",
    "- `TOP_PACK` -> ~42% missing\n",
    "\n",
    "The strategy will be used is imputing with **Unknown** category:\n",
    "- This avoids dropping rows.\n",
    "- Allows the model to treat the missingness as a distinct category, which can be informative.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0dd282da",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['REGION', 'TOP_PACK']\n",
    "\n",
    "for col in categorical_features:\n",
    "    data[col].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f29a28f",
   "metadata": {},
   "source": [
    "## Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b8c040",
   "metadata": {},
   "source": [
    "### 1. Identify categorical variables\n",
    "\n",
    "After dropping ``MRG`` and handling the missing data, the categorical data are: `REGION` and `TOP_PACK`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693424f",
   "metadata": {},
   "source": [
    "### 2. Encoding strategy\n",
    "\n",
    "- `REGION` -> One-hot encoding. It creates a separate binary column for each region allowing the model to treat each region independently.\n",
    "- `TOP_PACK` -> Frequency encoding. It has many unique values, and one-hot encoding would create too many columns, making the model complex and sparse. Frequency encoding, on the other hand, preserves information about popularity of each pack while keeping the feature numeric.\n",
    "- `TENURE` -> Ordinal numeric encoding. Tenure represents the range of duration in the network, so it has a natural order (shorter tenure -> higher likelihood of churn). Mapping to numeric values preserves this order and makes it usable in any model later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18182306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of REGION columns after encoding: 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. REGION → One-hot encoding\n",
    "data = pd.get_dummies(data, columns=['REGION'], prefix='REGION')\n",
    "\n",
    "print(f\"Number of REGION columns after encoding: {len([c for c in data.columns if 'REGION_' in c])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4df086f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example encoded values for TOP_PACK_FE:\n",
      "0    152295\n",
      "1    902594\n",
      "2     18454\n",
      "3     14629\n",
      "4     67512\n",
      "5     64412\n",
      "6    902594\n",
      "7    317802\n",
      "8    902594\n",
      "9     22332\n",
      "Name: TOP_PACK_FE, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. TOP_PACK → Frequency encoding\n",
    "top_pack_counts = data['TOP_PACK'].value_counts()\n",
    "data['TOP_PACK_FE'] = data['TOP_PACK'].map(top_pack_counts)\n",
    "data = data.drop(columns=['TOP_PACK'])\n",
    "print(f\"Example encoded values for TOP_PACK_FE:\\n{data['TOP_PACK_FE'].head(10)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f58e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example encoded values for TENURE_NUM:\n",
      "0    8\n",
      "1    6\n",
      "2    8\n",
      "3    8\n",
      "4    8\n",
      "5    8\n",
      "6    8\n",
      "7    8\n",
      "8    8\n",
      "9    8\n",
      "Name: TENURE_NUM, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. TENURE → Ordinal numeric encoding\n",
    "tenure_map = {\n",
    "    'D 3-6 month': 3,\n",
    "    'E 6-9 month': 6,\n",
    "    'F 9-12 month': 9,\n",
    "    'G 12-15 month': 12,\n",
    "    'H 15-18 month': 15,\n",
    "    'I 18-21 month': 18,\n",
    "    'J 21-24 month': 21,\n",
    "    'K > 24 month': 24,\n",
    "}\n",
    "data['TENURE_NUM'] = data['TENURE'].map(tenure_map)\n",
    "data = data.drop(columns=['TENURE'])\n",
    "\n",
    "print(f\"Example encoded values for TENURE_NUM:\\n{data['TENURE_NUM'].head(10)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b1eef78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTANT</th>\n",
       "      <th>FREQUENCE_RECH</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>ARPU_SEGMENT</th>\n",
       "      <th>FREQUENCE</th>\n",
       "      <th>DATA_VOLUME</th>\n",
       "      <th>ON_NET</th>\n",
       "      <th>ORANGE</th>\n",
       "      <th>TIGO</th>\n",
       "      <th>REGULARITY</th>\n",
       "      <th>...</th>\n",
       "      <th>REGION_LOUGA</th>\n",
       "      <th>REGION_MATAM</th>\n",
       "      <th>REGION_SAINT-LOUIS</th>\n",
       "      <th>REGION_SEDHIOU</th>\n",
       "      <th>REGION_TAMBACOUNDA</th>\n",
       "      <th>REGION_THIES</th>\n",
       "      <th>REGION_Unknown</th>\n",
       "      <th>REGION_ZIGUINCHOR</th>\n",
       "      <th>TOP_PACK_FE</th>\n",
       "      <th>TENURE_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4250.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4251.0</td>\n",
       "      <td>1417.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>152295</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>902594</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3600.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>18454</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13500.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13502.0</td>\n",
       "      <td>4501.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>43804.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14629</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>985.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>67512</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MONTANT  FREQUENCE_RECH  REVENUE  ARPU_SEGMENT  FREQUENCE  DATA_VOLUME  \\\n",
       "0   4250.0            15.0   4251.0        1417.0       17.0          4.0   \n",
       "1   3000.0             7.0   3000.0        1000.0        9.0        257.0   \n",
       "2   3600.0             2.0   1020.0         340.0        2.0        257.0   \n",
       "3  13500.0            15.0  13502.0        4501.0       18.0      43804.0   \n",
       "4   1000.0             1.0    985.0         328.0        1.0        257.0   \n",
       "\n",
       "   ON_NET  ORANGE  TIGO  REGULARITY  ...  REGION_LOUGA  REGION_MATAM  \\\n",
       "0   388.0    46.0   1.0          54  ...         False         False   \n",
       "1    27.0    29.0   6.0           4  ...         False         False   \n",
       "2    90.0    46.0   7.0          17  ...         False         False   \n",
       "3    41.0   102.0   2.0          62  ...         False         False   \n",
       "4    39.0    24.0   6.0          11  ...         False         False   \n",
       "\n",
       "   REGION_SAINT-LOUIS  REGION_SEDHIOU  REGION_TAMBACOUNDA  REGION_THIES  \\\n",
       "0               False           False               False         False   \n",
       "1               False           False               False         False   \n",
       "2               False           False               False         False   \n",
       "3               False           False               False         False   \n",
       "4               False           False               False         False   \n",
       "\n",
       "   REGION_Unknown  REGION_ZIGUINCHOR  TOP_PACK_FE  TENURE_NUM  \n",
       "0           False              False       152295           8  \n",
       "1            True              False       902594           6  \n",
       "2            True              False        18454           8  \n",
       "3           False              False        14629           8  \n",
       "4           False              False        67512           8  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c7f8e",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8e10f",
   "metadata": {},
   "source": [
    "1. ``REVENUE / MONTANT``: measures efficiency of revenue relative to top-ups. High ratio may indicate high-value customers.\n",
    "2. ``TENURE / FREQUENCE_RECH``: normalize recharge frequency by how long the customer has been active. Low value may indicate inactivity relative to tenure.\n",
    "3. ``TENURE / REGULARITY``: shows churn risk relative to engagement. Customers with low regularity relative to tenure may be more likely to churn.\n",
    "4. ``DATA_VOLUME / REGULARITY``: average data usage per active day. Captures engagement intensity.\n",
    "5. ``ON_NET / REGULARITY``: measures on-network calling frequency per active period. High value may indicate loyal users.\n",
    "6. ``REVENUE - MONTANT``: difference between revenue and top-up. Large posiitive or negative deviations could indicate unusual behaviour.\n",
    "7. Log-transformed numeric features: reduces skewness, helps models detect patterns across scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "317a8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['REV_DIV_MONTANT'] = data['REVENUE'] / (data['MONTANT'] + 1)  # +1 to avoid division by zero\n",
    "data['TENURE_DIV_FREQ_RECH'] = data['TENURE_NUM'] / (data['FREQUENCE_RECH'] + 1)\n",
    "data['TENURE_DIV_REG'] = data['TENURE_NUM'] / (data['REGULARITY'] + 1)\n",
    "data['DATA_DIV_REG'] = data['DATA_VOLUME'] / (data['REGULARITY'] + 1)\n",
    "data['ON_NET_DIV_REG'] = data['ON_NET'] / (data['REGULARITY'] + 1)\n",
    "data['REV_MINUS_MONTANT'] = data['REVENUE'] - data['MONTANT']\n",
    "\n",
    "numeric_cols = ['MONTANT','REVENUE','FREQUENCE','DATA_VOLUME','FREQUENCE_RECH','REGULARITY']\n",
    "for col in numeric_cols:\n",
    "    data[f'{col}_log'] = np.log1p(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cedcaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## draw the heatmap again and see the correlation, then decide finally on which features to drop/keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd572729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed data before scaling\n",
    "data_unscaled = data.copy()\n",
    "data_unscaled.to_csv('data/data_preprocessed_unscaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924be9b8",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbb56f6",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a957eb5",
   "metadata": {},
   "source": [
    "Split 1: Train (70%) + Temp (30%) (Test + Validation)\n",
    "\n",
    "Split 2: Validation (15%) + Test (15%) (from Temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e5a77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b018775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1507833, 40) (1507833,)\n",
      "Validation shape: (323107, 40) (323107,)\n",
      "Test shape: (323108, 40) (323108,)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=['CHURN'], axis=1)\n",
    "y = data['CHURN']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y) # 85% train, 15% test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42, stratify=y_train) # 70% train, 15% test\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc3ee59",
   "metadata": {},
   "source": [
    "## Scaling / Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "951c9c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "\n",
    "X_val_scaled[num_cols] = scaler.transform(X_val[num_cols])\n",
    "X_test_scaled[num_cols] = scaler.transform(X_test[num_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a2c56",
   "metadata": {},
   "source": [
    "# Model Experimentation\n",
    "\n",
    "Models that are suitable for churn prediction (binary classification):\n",
    "1. **Logistic Regression** (baseline linear model, sensitive to scaling).\n",
    "2. **Gradient Boosting models** (e.g., XGBoost, LightGBM), powerful tree-based models, handle non-linearities and missing values well.\n",
    "3. **Random Forest** (robust ensemble, less sensitive to scaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af695f87",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "Before evaluating, use the following:\n",
    "1. K-fold cross-validation on the training set to estimate the performance.\n",
    "2. Validation set for hyperparameter tuning. --> EXTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce635a",
   "metadata": {},
   "source": [
    "## Fit models\n",
    "\n",
    "* Fit the models on the training data (or training folds when using CV).\n",
    "* Evaluate on the validation set for tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707337f",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score\n",
    "* ROC_AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29c745",
   "metadata": {},
   "source": [
    "Then if time helps we do either of the following:\n",
    "\n",
    "a. Pick the best performed model and apply **feature selection/dimensionality reduction** and train the model again.\n",
    "\n",
    "b. Apply **feature selection/dimensionality reduction** and try it out on all models again.\n",
    "\n",
    "+ If the performance is still low then we consider handling the class imbalance: SMOTE, undersampling, or class weights.\n",
    "\n",
    "How the improvement or consideration should be for each model:\n",
    "1. Logistic Regression: --> Scaling is important\n",
    "    * **Hyperparameter tuning**: GridSearchCV. Key hyperparameters are: c (inverse of regularization strength), penalty (l1, l2, elasticnet), and solver (depedning on the penalty).\n",
    "    * **Feature selection/dimensionality reduction**: It is very compatible with L1 regulaization (lasso) for automatic feature selection. Also, it works well with PCA to reduce correlated features.\n",
    "    * **Class imbalance**: Use `class_weight='balanced'` to automatically adjust weights. OR, use SMOTE before fitting on the logistic regression.\n",
    "\n",
    "2. Gradient Boosting (XGBoost, LightGBM): --> Scaling not important\n",
    "    * **Hyperparameter tuning**: Effective. GridSearchCV. Key hyperparameters are: `n_estimators`, `max_depth`, `learning_rate`, `subsample`, `colsample_bytree`, `min_child_weight`. NOTE: if tuned well, it can get big performance.\n",
    "    * **Feature selection/dimensionality reduction**: Less critical + can compute feature importance after training for insight.\n",
    "    * **Class imbalance**: For XGBoost use `scale_pos_weight` and for LightGBM use `is_unbalance=True` or `class_weight='balanced`. SMOTE works but usually gradient boosting handles teh imbalance well with class weight.\n",
    "\n",
    "3. Random Forest: --> Scaling not important\n",
    "    * **Hyperparameter tuning**: GridSearchCV. Key hyperparameters are: `n_estimators`, `max_depth`, `max_features`, `min_samples_split`, `min_samples_leaf`. \n",
    "    * **Feature selection/dimensionality reduction**: Trees are robust, but it can be used to drop weak features.\n",
    "    * **Class imbalance**: Use `class_weight='balanced`. NOTE: SMOTE is optional but not important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50505e55",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
