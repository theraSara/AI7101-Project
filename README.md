# Customer (Expresso) Churn Prediction

## Overview
This project aims to predict **customer churn** (whether a customer will leave the service) using machine learning techniques.  
Churn prediction is crucial for telecom and subscription-based businesses because it allows proactive retention campaigns, reduces revenue loss, and lowers acquisition costs.

We experiment with three families of models:
- **Logistic Regression** (linear baseline, sensitive to scaling)
- **Random Forest** (tree-based ensemble, robust to noise and imbalance)
- **Gradient Boosting** (XGBoost/LightGBM, powerful non-linear learners)

We also evaluate multiple **feature selection strategies** (Filter, Wrapper, Embedded) and compare their impact on predictive performance.

---

##  Team
- **Sara**  
- **Mai**  
- **Yining**

---

## Project Structure
```bash
AI7101-Project/
â”‚
â”œâ”€â”€ data/ # Datasets
â”‚ â”œâ”€â”€ raw/ # Original input data (Train.csv, Test.csv, etc.)
â”‚ â”œâ”€â”€ processed/ # Preprocessed splits (train/val/test)
â”‚
â”œâ”€â”€ notebooks/ # Notebooks for step-by-step workflow
â”‚ â”œâ”€â”€ 01_data-exploration.ipynb
â”‚ â”œâ”€â”€ 02_data-preprocessing.ipynb
â”‚ â”œâ”€â”€ 03_data-evaluation.ipynb
â”‚
â”œâ”€â”€ src/churn/ # Source code
â”‚ â”œâ”€â”€ experiments/ # Experiment scripts (FS + CV runs)
â”‚ â”‚ â”œâ”€â”€ logistic_regression_experiments.py
â”‚ â”‚ â”œâ”€â”€ random_forest_experiments.py
â”‚ â”‚ â”œâ”€â”€ xgboost_experiments.py
â”‚ â”‚
â”‚ â”œâ”€â”€ train/ # Training scripts for baseline models
â”‚ â”‚ â”œâ”€â”€ train_logistic_regression.py
â”‚ â”‚ â”œâ”€â”€ train_random_forest.py
â”‚ â”‚ â”œâ”€â”€ train_xgboost.py
â”‚ â”‚
â”‚ â”œâ”€â”€ eda.py # Exploratory data analysis helpers
â”‚ â”œâ”€â”€ eval.py # Unified evaluation (ROC, PR, confusion, importances)
â”‚ â”œâ”€â”€ utils.py # Shared utilities (splits, seeding, metrics)
â”‚ â”œâ”€â”€ init.py
â”‚
â”œâ”€â”€ models/ # Saved models + metrics (joblib/json)
â”œâ”€â”€ figs/ # Exported figures (ROC/PR curves, feature importances)
â”œâ”€â”€ README.md # Documentation
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ .gitattributes / .gitignore
```

---
## Dataset
- **Source**: Telecom churn dataset provided for AI7101 coursework.
- **Size**: ~2.1M records, 18 features.
- **Target**: `CHURN` (binary: 0 = stay, 1 = churn).
- **Important preprocessing decisions**:
  - Dropped features with >90% missing (`ZONE1`, `ZONE2`), or no variance (`MRG`).
  - Imputed **numeric** features with median, **categorical** with `"Unknown"`.
  - Encoded:
    - `REGION` â†’ one-hot encoding.
    - `TOP_PACK` â†’ frequency encoding.
    - `TENURE` â†’ ordinal mapping.
  - Feature engineering:
    - Ratios such as `REVENUE / MONTANT`, `DATA_VOLUME / REGULARITY`.
    - Differences such as `REVENUE - MONTANT`.
    - Log-transform of skewed numeric variables.

---

## Exploratory Data Analysis (EDA)
- Visualized **target distribution** â†’ Imbalanced dataset.
- **Missing values** analysis (bar plots).
- **Categorical vs churn** distributions (count plots).
- **Numeric distributions** (histograms + boxplots).
- **Correlations** (heatmap: `REVENUE` and `ARPU_SEGMENT` are collinear).
- Applied **log transforms** for skewed features.

---

## Modeling
### Algorithms
- **Logistic Regression** (baseline, requires scaling).
- **Random Forest** (tree ensemble).
- **XGBoost / Gradient Boosting** (powerful boosting model).

### Feature Selection Methods
- **Filter**: `SelectKBest` (ANOVA F-test).
- **Wrapper**: Recursive Feature Elimination (RFE).
- **Embedded**: SelectFromModel (based on feature importance or L1 regularization).

### Evaluation
- Proper **train/val/test split**:
  - Train 70%, Validation 15%, Test 15% (stratified).
- **Cross-validation** for robustness.
- Metrics reported:
  - ROC AUC
  - PR AUC
  - Precision, Recall, F1-score
- Threshold optimized on validation by **F1-score**.

---

## Results
- **Random Forest (tuned baseline)**: ROC AUC â‰ˆ 0.93, PR AUC â‰ˆ 0.70, F1 â‰ˆ 0.69.
- Feature selection (filter, wrapper, embedded) gave **marginal improvements**.
- **Logistic Regression**: Competitive baseline (ROC AUC â‰ˆ 0.92) but lower F1 due to imbalance.
- **XGBoost**: Best performance overall (ROC AUC â‰ˆ 0.9315, PR AUC â‰ˆ 0.706, F1 â‰ˆ 0.68).

ðŸ“Š Figures (saved in `/figs`):
- ROC and PR curves (Validation & Test).
- Confusion matrices.
- Feature importance plots.

---

## How to Run
### 1. Install dependencies
```bash
conda create -n churn python=3.11
conda activate churn
pip install -r requirements.txt
```
---

### 2. Preprocess data

```bash
jupyter notebook notebooks/02_data-preprocessing.ipynb
```
---

### 3. Train models

Random Forest:
```bash
python -m src.churn.random_forest_experiments --data_dir data/processed --models_dir models --cv 3 --jobs 2 --run_filter --run_wrapper --run_embedded
```

Logistic Regression:
```bash
python -m src.churn.logistic_regression_experiments --data_dir data/processed --models_dir models --cv 3 --jobs 2 --run_filter --run_wrapper --run_embedded
```

XGBoost:
```bash
python -m src.churn.xgboost_experiments --data_dir data/processed --models_dir models --cv 3 --jobs 2 --run_filter --run_wrapper --run_embedded
```

---

### 4. Evaluate models
Open:
```bash
notebooks/03_data-evaluation.ipynb
```
to generate classification reports, confusion matrices, ROC/PR curves, and feature importance plots.

---

### Contributors:
* Mai Chau
* Sara Alhajeri
* Yining Ma